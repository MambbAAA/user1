import numpy as np

# Input data
X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float)

# Normalize the input data to bring them within the range 0 to 1
X = X / np.amax(X, axis=0)
y = y / 100

# Sigmoid Activation Function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative of Sigmoid Function
def sigmoid_derivative(x):
    return x * (1 - x)

# Variable initialization
epochs = 5000  # Number of training iterations
lr = 0.1

# Input, hidden, and output neurons
in_n = 2
hid_n = 3
out_n = 1

# Weight and bias initialization
ihw = np.random.uniform(size=(in_n, hid_n))
hb = np.random.uniform(size=(1, hid_n))
how = np.random.uniform(size=(hid_n, out_n))
ob = np.random.uniform(size=(1, out_n))

# Training the neural network
for i in range(epochs):
    # Forward Propagation
    hi = np.dot(X, ihw) + hb
    hlo = sigmoid(hi)  # HIDDEN LAYER OUTPUT
    oi = np.dot(hlo, how) + ob
    output = sigmoid(oi)

    # Backpropagation
    oe = y - output  # OUTPUT ERROR
    od = oe * sigmoid_derivative(output)  # OUTPUT DELTA
    he = od.dot(how.T)  # HIDDEN ERROR
    hd = he * sigmoid_derivative(hlo)  # HIDDEN DELTA

    # Updating the weights and biases using Gradient Descent
    how += hlo.T.dot(od) * lr  # HIDDEN OUTPUT WEIGHTS
    ihw += X.T.dot(hd) * lr  # INPUT HIDDEN WEIGHTS

print("Input: \n" + str(X))
print("Actual Output: \n" + str(y))
print("Predicted Output: \n", output)
